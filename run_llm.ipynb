{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7f42409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './src')  # 或者绝对路径：/full/\n",
    "from src.xmcdata import *\n",
    "from llm import ModelConfig,LLMTrainer,KeyphrasePredictor,DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7278e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'eurlex-4k'\n",
    "data_dir = f\"xmc-base/{dataset_name}\"\n",
    "\n",
    "label_map = load_label_text_map(data_dir + \"/output-items.txt\")\n",
    "\n",
    "# training dataset\n",
    "X_trn_text = load_texts(data_dir+\"/X.trn.txt\")\n",
    "Y_trn_feat = load_sparse_matrix(data_dir+\"/Y.trn.npz\")\n",
    "\n",
    "Y_trn_text,Y_trn_num = csr_id_to_text(Y_trn_feat,label_map)\n",
    "\n",
    "Y_trn_list= [\",\".join(y) for y in Y_trn_text]\n",
    "\n",
    "# validation dataset\n",
    "X_tst_text = load_texts(data_dir+\"/X.tst.txt\")\n",
    "Y_tst_feat = load_sparse_matrix(data_dir+\"/Y.tst.npz\")\n",
    "\n",
    "Y_tst_text, Y_tst_num = csr_id_to_text(Y_tst_feat,label_map)\n",
    "Y_tst_list = [\",\".join(y) for y in Y_tst_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "751d5dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_input_template = \"Summarize the following document with keyphrases:\\n\\nDocument: {document}\"\n",
    "normal_input_template = \"Summarize the following document with keyphrases:\\n\\nDocument: {document}\"\n",
    "stemmed_output_template = \"Summary of this paragraph by unstemmed keyphrases: {keyphrases}\"  # 输出模板\n",
    "output_template = \"Summary of this paragraph by keyphrases: {keyphrases}\"  # 输出模板\n",
    "model_name = \"unsloth/Llama-3.2-3B-Instruct\"  # 可以替换为其他模型如 \"meta-llama/Llama-2-7b-hf\"\n",
    "llm_train_config = ModelConfig(\n",
    "    model_name=model_name,  # 可以替换为其他模型如 \"meta-llama/Llama-2-7b-hf\"\n",
    "    max_length=512,\n",
    "    batch_size=2,\n",
    "    learning_rate=2e-4,\n",
    "    num_epochs=3,\n",
    "    use_quantization=True,\n",
    "    quantization_type=\"fp16\",  # 可选: \"int4\", \"int8\", \"fp16\", \"fp32\"\n",
    "    output_dir=\"./ouput/\"+dataset_name,\n",
    "    lora_r= 16,\n",
    "    lora_alpha= 32,\n",
    "    lora_dropout= 0.1,\n",
    "    prompt_template=stemmed_input_template,\n",
    "    max_new_tokens = 128 # 生成的最大新令牌数\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83deb329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428a6eec99374fd39db6d2d176ed71bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n",
      "trainable params: 24,313,856 || all params: 3,237,063,680 || trainable%: 0.7511083625021551\n",
      "LoRA configuration applied successfully!\n"
     ]
    }
   ],
   "source": [
    "stemmed_input_template = \"Summarize the following document with keyphrases:\\n\\nDocument: {document}\"\n",
    "normal_input_template = \"Summarize the following document with keyphrases:\\n\\nDocument: {document}\"\n",
    "stemmed_output_template = \"Summary of this paragraph by unstemmed keyphrases: {keyphrases}\"  # 输出模板\n",
    "output_template = \"Summary of this paragraph by keyphrases: {keyphrases}\"  # 输出模板\n",
    "trainer = LLMTrainer(llm_train_config)\n",
    "#加载模型\n",
    "# setting lora \n",
    "trainer.setup_lora()\n",
    "# prepare dataset\n",
    "data_processor = DataProcessor(tokenizer=trainer.tokenizer,max_length_input=384,\n",
    "                                max_length_output=128,  # 输出的最大长度\n",
    "                                max_length = trainer.config.max_length,  # 输入的最大长度\n",
    "                                prompt_template = stemmed_input_template,\n",
    "                                res_template = stemmed_output_template\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab4837a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0d4a28772044569e2b5fcf0527b45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset (num_proc=8):   0%|          | 0/15449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342395130e2743e493ad5309b6c61573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/15449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc4135be3da414bbeb021ecf40fbbd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing dataset (num_proc=8):   0%|          | 0/3865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ddfd6c93d6942acbd4750f67760dc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/3865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f5f43a8cb741aba330ad58882ede46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/15449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to xmc-base/eurlex-4k/train_dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54993a2f61624468ab9e8454e2feb472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to xmc-base/eurlex-4k/val_dataset\n"
     ]
    }
   ],
   "source": [
    "train_dataset = data_processor.prepare_dataset(documents=X_trn_text,\n",
    "                                               keyphrases=Y_trn_list,num_proc=8)\n",
    "val_dataset = data_processor.prepare_dataset(documents=X_tst_text,\n",
    "                                             keyphrases=Y_tst_list,num_proc=8)\n",
    "data_processor.save_dataset(train_dataset, data_dir+\"/train_dataset\")\n",
    "data_processor.save_dataset(val_dataset, data_dir+\"/val_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b947fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded from xmc-base/eurlex-4k/train_dataset\n",
      "Dataset loaded from xmc-base/eurlex-4k/val_dataset\n"
     ]
    }
   ],
   "source": [
    "train_dataset = data_processor.load_dataset(data_dir+\"/train_dataset\")\n",
    "val_dataset = data_processor.load_dataset(data_dir+\"/val_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0572636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model from ./output/eurlex-4k/unsloth/Llama-3.2-3B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218bc8e1ca0a4aa8a0bee2ae1d9afa98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "output_dir=f\"./output/{dataset_name}/{model_name}\"\n",
    "trainer.load_trained_model(model_path=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c15b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrasePredictor = KeyphrasePredictor(trainer=trainer)\n",
    "res = keyphrasePredictor.predict(documents=X_tst_text[0:2],max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44f01d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Summarize the following document with keyphrases:\\nDocument: commiss decis juli lai detail rule applic franc articl regul ec special market support measur beef sector notifi document number french text authent ec commiss european commun regard treati establish european commun regard council regul ec common organis market beef veal articl thereof regard commiss regul ec april special market support measur beef sector articl thereof articl regul ec releas approv commiss meat bought regul laid releas specif commun made avoid disturb market direct competit dispos intervent product applic articl indent subparagraph articl regul ec franc bought quantiti beef franc continu bui meat partial tender end year franc submit request commiss releas tonn beef bought scheme intend distribut depriv person franc meet elig criteria appli commiss regul eec octob lai detail rule suppli food intervent stock benefit depriv person commun amend regul ec request approv releas follow procedur laid indent articl regul eec releas direct consumpt depriv person disturb market compet dispos intervent product french request met order disturb market releas product intend payment kind product compris meat hindquart pistola hindquart maximum rib measur provid decis accord opinion manag committe beef veal adopt decis articl applic articl regul ec franc authoris releas tonn express product weight beef purchas regul provid releas product consumpt directli process depriv person meet elig criteria laid franc pursuant articl regul eec provis indent articl regul eec appli regard releas refer paragraph applic thereof franc authoris product concern payment kind except meat hindquart purpos decis hindquart refer pistola hindquart maximum rib articl decis address french republ brussel juli commiss franz fischler member commiss oj oj oj oj\\nSummary of this paragraph by unstemmed keyphrases: beef,food aid,france,intervention stock,meat product,poor person,voluntary organisation,work of charity,world food day,world food prize,young person,zaouy,zeigler,zSummary of this paragraph by unstemmed keyphrases: aid system,aid to agriculture,beef,france,meat product,voluntary organisation,zaouy,zeigler,world food prize,world food day,food aid,foodstuff,intervention stock,purchase,poor person,work of charity,discount',\n",
       " 'Summarize the following document with keyphrases:\\nDocument: agre minut european commun govern republ korea relat world shipbuild market commiss european commun govern republ korea held talk march address current problem relat world shipbuild market view promot stabil fair competit side recognis world shipbuild market characteris signific overcapac steadili decreas price allow sustain develop industri european commun korea main player shipbuild market respect author special oblig work shipbuild nation ensur fair competit prevail market side expect achiev object contribut major restor normal competit condit market provid effect mean protect sale ship price cost side work view reduc unsustain prevail imbal suppli demand invit shipbuild countri support effort firmli commit fair competit parti avoid financi viabl invest ruinou price undercut side make effort individu basi jointli improv stabilis market situat aim side promot fair competit market condit world market work stabilis market rais level ship price commerci sustain action public author respect shipyard financi difficulti side agre financi institut conduct busi shipbuild commerci sound manner respect korean govern continu supervis strictli asset sound financi institut line korean govern polici intervent consciou imbal world shipbuild market korean author ensur context bank supervis bank korean govern sharehold privat bank act behalf extend loan write roll exist loan provid type support commerci basi korean govern confirm provid financi institut public support purpos cover loss result busi relat specif enterpris industri korean govern agre kamco purchas bad loan relat shipyard price reflect actual expect recoveri rate fund cost minim price unsecur loan korean govern confirm extend support shipbuild inconsist korea intern oblig manag takeov samho hyundai accompani publicli support debt restructur moratoria oper govern ownership korean bank deal shipbuild compani oper fulli commerci basSummary of this paragraph by unstemmed keyphrases: competition,financial aid,shipbuilding,south korea,world market price,world trade market,youth employment policy,world market price,world trade market,world market price,world trade market,world trade market,world trade market,world market price,world market price,world market price,world market price,worldSummary of this paragraph by unstemmed keyphrases: competition,financial aid,shipbuilding,south korea,world market price,world trade market,world market price,world trade market,world market price,worldSummary of this paragraph by un']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ca0fa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checked 15449 samples. Found 0 with all -100 labels.\n",
      "✅ Checked 3865 samples. Found 0 with all -100 labels.\n"
     ]
    }
   ],
   "source": [
    "def check_all_labels_ignored(dataset, sample_size:int|None =10):\n",
    "    \"\"\"\n",
    "    检查 dataset 中是否存在 labels 全为 -100 的样本\n",
    "    如果 sample_size=None，则检查整个 dataset\n",
    "    \"\"\"\n",
    "    total = len(dataset)\n",
    "    check_range = range(total) if sample_size is None else range(min(sample_size, total))\n",
    "    error_count = 0\n",
    "\n",
    "    for i in tqdm(check_range):\n",
    "        labels = dataset[i][\"labels\"]\n",
    "        if isinstance(labels, list):\n",
    "            labels_tensor = torch.tensor(labels)\n",
    "        elif isinstance(labels, torch.Tensor):\n",
    "            labels_tensor = labels\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported label type: {type(labels)}\")\n",
    "\n",
    "        if (labels_tensor != -100).sum() == 0:\n",
    "            print(f\"⚠️ Warning: Sample {i} has all labels == -100\")\n",
    "            error_count += 1\n",
    "\n",
    "    print(f\"✅ Checked {len(check_range)} samples. Found {error_count} with all -100 labels.\")\n",
    "\n",
    "check_all_labels_ignored(train_dataset, sample_size=None)\n",
    "check_all_labels_ignored(val_dataset, sample_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2844168a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Forward 输出 ===\n",
      "loss: tensor(4.9695, device='cuda:0')\n",
      "loss.requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "# 1. 取一个 batch 样本\n",
    "sample = train_dataset[0]\n",
    "sample = {k: torch.tensor(v).unsqueeze(0).to(trainer.model.device) for k, v in sample.items()}\n",
    "\n",
    "# 2. 执行 forward\n",
    "output = trainer.model(**sample)\n",
    "\n",
    "# 3. 检查输出\n",
    "print(\"=== Forward 输出 ===\")\n",
    "print(\"loss:\", output.loss)\n",
    "print(\"loss.requires_grad:\", output.loss.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719bb006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.model.training:  True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2456' max='23175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2456/23175 26:50 < 3:46:39, 1.52 it/s, Epoch 0.32/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.861900</td>\n",
       "      <td>0.838727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.708600</td>\n",
       "      <td>0.727005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.616000</td>\n",
       "      <td>0.699808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.687900</td>\n",
       "      <td>0.648379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 保存模型\u001b[39;00m\n\u001b[1;32m      3\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(save_path\u001b[38;5;241m=\u001b[39mllm_train_config\u001b[38;5;241m.\u001b[39moutput_dir)\n",
      "File \u001b[0;32m/workspace/xrmodel/./src/llm.py:273\u001b[0m, in \u001b[0;36mLLMTrainer.train\u001b[0;34m(self, train_dataset, eval_dataset)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# 必须显式关闭\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# 创建trainer\u001b[39;00m\n\u001b[1;32m    263\u001b[0m trainer \u001b[38;5;241m=\u001b[39m PeftTrainer(\n\u001b[1;32m    264\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    265\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m    266\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m    267\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,\n\u001b[1;32m    268\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m    269\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[1;32m    270\u001b[0m     load_best_model_at_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    271\u001b[0m     save_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    272\u001b[0m     save_total_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 273\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself.model.training: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/trainer.py:2555\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2548\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2549\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2553\u001b[0m )\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2555\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2558\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2561\u001b[0m ):\n\u001b[1;32m   2562\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2563\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/trainer.py:3791\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   3789\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 3791\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3793\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/accelerate/accelerator.py:2469\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2468\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2469\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/lib/python3.10/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(train_dataset,val_dataset,llm_train_config.output_dir)\n",
    "# 保存模型\n",
    "trainer.save_model(save_path=llm_train_config.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4ebe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preictor = KeyphrasePredictor(trainer=trainer)\n",
    "# 预测\n",
    "predictions = preictor.predict(X_tst_text)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa23eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存预测结果\n",
    "with open(data_dir +\"/pred_\"+ model_name + \".txt\", \"w\") as f:\n",
    "    for pred in predictions:\n",
    "        f.write(f\"Predicted Keyphrases: {pred}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
